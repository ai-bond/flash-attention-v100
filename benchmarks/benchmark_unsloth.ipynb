{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23286fa1-8c01-4942-80fe-ea60c8140c16",
   "metadata": {},
   "source": [
    "### Direct replace unsloth's FA2 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777fc0f-6602-4dcf-bf71-9951d0c0fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"UNSLOTH_USE_FLASH_ATTENTION\"] = \"force\"\n",
    "\n",
    "import sys, types, importlib.util\n",
    "\n",
    "def make_package(name):\n",
    "    m = types.ModuleType(name)\n",
    "    m.__path__ = []\n",
    "    m.__spec__ = importlib.util.spec_from_loader(name, loader=None, is_package=True)\n",
    "    m.__package__ = name\n",
    "    return m\n",
    "\n",
    "def make_module(name):\n",
    "    m = types.ModuleType(name)\n",
    "    m.__spec__ = importlib.util.spec_from_loader(name, loader=None)\n",
    "    m.__package__ = name.rpartition(\".\")[0]\n",
    "    return m\n",
    "\n",
    "from flash_attn_v100 import flash_attn_func\n",
    "\n",
    "fa = make_package(\"flash_attn\")\n",
    "fa.flash_attn_func = flash_attn_func\n",
    "sys.modules[\"flash_attn\"] = fa\n",
    "\n",
    "fai = make_module(\"flash_attn.flash_attn_interface\")\n",
    "fai.flash_attn_func = flash_attn_func\n",
    "fai.flash_attn_varlen_func = flash_attn_func\n",
    "sys.modules[\"flash_attn.flash_attn_interface\"] = fai\n",
    "\n",
    "fbp = make_module(\"flash_attn.bert_padding\")\n",
    "fbp.index_first_axis = fbp.pad_input = fbp.unpad_input = lambda x, *a, **k: x\n",
    "sys.modules[\"flash_attn.bert_padding\"] = fbp\n",
    "\n",
    "print(\"ðŸ¦¥ Volta shim installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0f8ce-ec4e-4313-84d3-67b4857bc5be",
   "metadata": {},
   "source": [
    "### Set unsloth params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6d73e-7d0f-4f9e-bd6a-e8ecca48c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"Your project name\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"Your api key\"\n",
    "\n",
    "hf_id    = \"Your Huggingface id\"\n",
    "hf_token = \"Your Huggingface api key\"\n",
    "\n",
    "# Choose HF model for tain\n",
    "original_model   = \"unsloth/llama-2-7b-bnb-4bit\"\n",
    "hf_model_split   = original_model.split(\"/\")\n",
    "hf_model_name    = hf_model_split[0]\n",
    "hf_model_name_id = hf_model_split[1]\n",
    "\n",
    "# Unsloth Model params\n",
    "max_seq_length = 4096\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "load_in_8bit = False\n",
    "full_finetuning = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ff9d0-555d-4857-b75d-2f890c685ae3",
   "metadata": {},
   "source": [
    "### Unsloth monkey patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f47199-ce7c-4e2b-937d-4686807303ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth.models._utils as _utils\n",
    "\n",
    "_utils.HAS_FLASH_ATTENTION = True\n",
    "_utils.HAS_FLASH_ATTENTION_SOFTCAPPING = False\n",
    "_utils.SUPPORTS_BFLOAT16 = False\n",
    "\n",
    "import unsloth.models.llama as llama_module\n",
    "import importlib\n",
    "importlib.reload(llama_module)\n",
    "\n",
    "if hasattr(llama_module, 'flash_attn_func'):\n",
    "    print(\"ðŸ¦¥ flash_attn_func is now in llama.py\")\n",
    "else:\n",
    "    llama_module.flash_attn_func = flash_attn_func\n",
    "    print(\"ðŸ¦¥  Manually patched llama.flash_attn_func\")\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = f\"{original_model}\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "print(\"ðŸ¦¥ Model loaded Volta FA2 is active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be49a9-1aa1-4e9a-82ad-953f94d7f764",
   "metadata": {},
   "source": [
    "### Unsloth lora params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26e3b0-3486-4c84-9d11-a7fc4a17c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,                                                                                         # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Add for continual pretraining\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,                                                                               # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",                                                                                  # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\",                                                         # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,                                                                             # We support rank stabilized LoRA\n",
    "    loftq_config = None,                                                                            # And LoftQ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddb5f2-79e9-4836-bec8-810daec4e3c5",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185fd74-965d-40b8-a747-b92ffd52ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "alpaca_prompt = \"\"\"You are an assistant that answers questions. An instruction is given describing a task, along with an input.\n",
    "\n",
    "1. Follow the instruction as precisely as possible: in terms of type, requested actions, phrasing, and considering the information from the input.\n",
    "2. In your response, always use the Russian language exclusively. Respond in a literate manner.\n",
    "3. In your response, do not use other languages.\n",
    "4. In your response, rely on knowledge from official sources. Consider Laws, encyclopedic knowledge, textbooks, books, official documents, court decisions, rulings, legislative acts.\n",
    "5. Do not generate random and/or fitting data. In the answer, do not use generic templates. Identify the main point of the question and provide a concise answer.\n",
    "6. The instruction might be without an input, in which case simply answer within the given context.\n",
    "7. Follow the same template as provided in the instruction.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = [\n",
    "        alpaca_prompt.format(instr, inp, out) + EOS\n",
    "        for instr, inp, out in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"])\n",
    "    ]\n",
    "    out_texts = []\n",
    "    for ids in tokenizer(texts, add_special_tokens=False).input_ids:\n",
    "        if len(ids) > max_seq_length - 1:\n",
    "            ids = ids[:max_seq_length - 1]\n",
    "        n = (len(ids) // 16) * 16\n",
    "        if n >= 15:\n",
    "            ids = ids[:n] + [tokenizer.eos_token_id or tokenizer.convert_tokens_to_ids(EOS)]\n",
    "            out_texts.append(tokenizer.decode(ids, skip_special_tokens=False))\n",
    "    return {\"text\": out_texts}\n",
    "\n",
    "ds = (\n",
    "    load_dataset(\"ai-bond/ru-alpaca-grandpro\", split=\"train[98%:]\")\n",
    "    .map(formatting_prompts_func, batched=True, remove_columns=[\"instruction\", \"input\", \"output\"])\n",
    "    .filter(lambda x: len(tokenizer.encode(x[\"text\"], add_special_tokens=False)) >= 16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ce008-de94-4d1a-8704-bf579778fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ds): print(\"[\", len(ds), \"tokens ] :\", repr(ds[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb2ea3-46d0-483d-975c-a66c9f7e1e83",
   "metadata": {},
   "source": [
    "### Trainer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e767116-8e2e-4647-97b0-0679cd67426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Use GA to mimic batch size!\n",
    "        warmup_steps=1,\n",
    "        # num_train_epochs = 1,         # Set this for 1 full training run.\n",
    "        max_steps=20,\n",
    "        learning_rate=2e-4,             # Reduce to 2e-5 for long training runs\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",               # Use this for WandB etc\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f21e55-6782-45b8-bdfa-81d42f2e5486",
   "metadata": {},
   "source": [
    "### Run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063b2ba-982f-4021-9b07-af8068b47f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from unsloth import unsloth_train\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start_event.record()\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)\n",
    "\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "gpu_time_ms = start_event.elapsed_time(end_event)\n",
    "print(f\"GPU time: {gpu_time_ms / 1000:.2f} sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
